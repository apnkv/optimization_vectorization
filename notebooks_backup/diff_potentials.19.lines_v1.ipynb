{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/apankov/dev.vectorization')\n",
    "from vectran.renderers.cairo import render as original_render, render_with_skeleton as original_render_with_skeleton, PT_LINE\n",
    "\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No NaN Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "class NonanAdam(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                grad[~torch.isfinite(grad)] = 0 ### <---\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                assert np.isfinite(step_size)\n",
    "                assert torch.all(torch.isfinite(exp_avg))\n",
    "                assert torch.all(torch.isfinite(denom))\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "division_epsilon = 1e-12\n",
    "min_linear = 2**-8\n",
    "empty_charge = 0\n",
    "visibility_padding = 2\n",
    "elementary_halfwidth = 1 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(data, dimensions, data_representation):\n",
    "    return original_render(data, dimensions, data_representation, linecaps='butt', linejoin='miter')\n",
    "\n",
    "\n",
    "def render_skeleton(data, dimensions, data_representation, padding=0, scaling=4):\n",
    "    w, h = dimensions\n",
    "    data[..., [0, 2]] = np.clip(data[..., [0, 2]], .5, w-.5)\n",
    "    data[..., [1, 3]] = np.clip(data[..., [1, 3]], .5, h-.5)\n",
    "    data[..., :4] += padding\n",
    "    data = data * [scaling, scaling, scaling, scaling, 1]\n",
    "    data[..., :4][data[..., -1] < 1/4] = -100\n",
    "    data[..., -1] = 0\n",
    "    return original_render_with_skeleton({PT_LINE: data}, [(w + padding*2)*scaling, (h + padding*2)*scaling], data_representation, linecaps='butt', linejoin='miter', line_color=(0,0,0), line_width=2)\n",
    "    \n",
    "\n",
    "def get_random_line(h, w, max_width=None):\n",
    "    if max_width is None:\n",
    "        max_width = max(h,w) // 10\n",
    "    line = np.float32(np.random.random(5) * [w, h, w, h, max_width])\n",
    "    line[..., -1] += 1\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable rasterizer with supersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupersamplingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def supersample(self, raster_coordinates):\n",
    "        \"\"\"Generate coordinates of supersamples for given coordinates of rasters.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def subsample(self, supersamples):\n",
    "        \"\"\"Average supersamples to get the actual raster\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reorder_supersamples(self, supersamples):\n",
    "        \"\"\"Reorder supersamples, generated for C ordered (row major) raster_coordinates, for being in C order.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        supersamples : torch.Tensor\n",
    "            Tensor of shape [**, h, w]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class RegularSupersampling(SupersamplingStrategy):\n",
    "    def __init__(self, supersamples_per_dimension=4):\n",
    "        self.supersamples_per_dimension = supersamples_per_dimension\n",
    "    \n",
    "    def supersample(self, raster_coordinates):\n",
    "        \"\"\"Generate coordinates of supersamples for given coordinates of rasters.\"\"\"\n",
    "        n = self.supersamples_per_dimension\n",
    "        rasters_n = raster_coordinates.shape[1]\n",
    "        n2 = n ** 2\n",
    "        supersample_coordinates = torch.empty([2, rasters_n * n2], dtype=raster_coordinates.dtype, device=raster_coordinates.device)\n",
    "        for i in range(n):\n",
    "            xs = raster_coordinates[0] + (i / n - (n-1) / (n * 2))\n",
    "            ys = raster_coordinates[1] + (i / n - (n-1) / (n * 2))\n",
    "            for j in range(n):\n",
    "                supersample_coordinates[0, i + n * j :: n2] = xs\n",
    "                supersample_coordinates[1, i * n + j :: n2] = ys\n",
    "        del xs, ys\n",
    "        return supersample_coordinates\n",
    "\n",
    "    def subsample(self, supersamples, dtype=torch.float32):\n",
    "        \"\"\"Average supersamples to get the actual raster\"\"\"\n",
    "        return torch.nn.functional.avg_pool1d(supersamples.type(dtype), self.supersamples_per_dimension**2)\n",
    "\n",
    "    def reorder_supersamples(self, supersamples):\n",
    "        \"\"\"Reorder supersamples, generated for C ordered (row major) raster_coordinates, for being in C order.\"\"\"\n",
    "        shape = supersamples.shape\n",
    "        other_dims = shape[:-2]\n",
    "        H = W = self.supersamples_per_dimension\n",
    "        hH, wW = shape[-2:]\n",
    "        assert ( hH % H == 0 ) and ( wW % W == 0 )\n",
    "        h = hH // H\n",
    "        w = wW // W\n",
    "        supersamples = supersamples.reshape(*other_dims, h, w, H, W).transpose(-3, -2)\n",
    "        return supersamples.reshape(shape)\n",
    "\n",
    "\n",
    "class RGSSSupersampling(SupersamplingStrategy):\n",
    "    \"\"\"Only 4xRGSS by now.\"\"\"\n",
    "\n",
    "    def supersample(self, raster_coordinates):\n",
    "        \"\"\"Generate coordinates of supersamples for given coordinates of rasters.\"\"\"\n",
    "        rasters_n = raster_coordinates.shape[1]\n",
    "        supersample_coordinates = torch.empty([2, rasters_n * 4], dtype=raster_coordinates.dtype, device=raster_coordinates.device)\n",
    "        supersample_coordinates[0, 0::4] = raster_coordinates[0] + 1/8\n",
    "        supersample_coordinates[1, 0::4] = raster_coordinates[1] - 3/8\n",
    "        supersample_coordinates[0, 1::4] = raster_coordinates[0] - 1/8\n",
    "        supersample_coordinates[1, 1::4] = raster_coordinates[1] - 1/8\n",
    "        supersample_coordinates[0, 2::4] = raster_coordinates[0] + 3/8\n",
    "        supersample_coordinates[1, 2::4] = raster_coordinates[1] + 1/8\n",
    "        supersample_coordinates[0, 3::4] = raster_coordinates[0] - 1/8\n",
    "        supersample_coordinates[1, 3::4] = raster_coordinates[1] + 3/8\n",
    "        return supersample_coordinates\n",
    "\n",
    "    def subsample(self, supersamples, dtype=torch.float32):\n",
    "        \"\"\"Average supersamples to get the actual raster\"\"\"\n",
    "        return torch.nn.functional.avg_pool1d(supersamples.type(dtype), 4)\n",
    "    \n",
    "    def reorder_supersamples(self, supersamples):\n",
    "        \"\"\"Reorder supersamples, generated for C ordered (row major) raster_coordinates, for being in C order.\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_lines(x1, y1, x2, y2, width, sample_coordinates, samples=None, linecaps='butt',\n",
    "                 dtype=torch.float16, requires_grad=False, sparse=False):\n",
    "    \"\"\"...\n",
    "    Requires 34 bytes of GPU memory per patch per line per sample for half precision computations (default),\n",
    "             45 for single precision computations, and 65 for double presision computations,\n",
    "     i.e 2.7/3.5/5 GB to render 128 patches of 10 lines in patches of size 64x64 with 4x4 supersampling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        samples : torch.BoolTensor\n",
    "    \"\"\"\n",
    "    # assert linecaps == 'butt', 'Others are not implemented by now (but easy to implement)'\n",
    "    # assert not requires_grad, 'Not implemented'\n",
    "    batch_size, lines_n = x1.shape\n",
    "    rasters_n = sample_coordinates.shape[1]\n",
    "    \n",
    "    with torch.set_grad_enabled(requires_grad):\n",
    "        l_dir = torch.empty(2, batch_size, lines_n, dtype=dtype, device=x1.device)\n",
    "        l_dir[0] = x2.type(dtype)\n",
    "        l_dir[0] -= x1.type(dtype)\n",
    "        l_dir[1] = y2.type(dtype)\n",
    "        l_dir[1] -= y1.type(dtype)\n",
    "        length = torch.norm(l_dir, dim=0)\n",
    "        l_dir /= (length + division_epsilon)\n",
    "\n",
    "        R = torch.empty(2, batch_size, lines_n, rasters_n, dtype=l_dir.dtype, device=l_dir.device)\n",
    "        R[0] = sample_coordinates[0]\n",
    "        R[0] -= x1[..., None].type(R.dtype)\n",
    "        R[1] = sample_coordinates[1]\n",
    "        R[1] -= y1[..., None].type(R.dtype)\n",
    "        \n",
    "        proj = l_dir[0, ..., None] * R[0]\n",
    "        proj += l_dir[1, ..., None] * R[1]\n",
    "        \n",
    "        shaded = proj <= length[..., None]\n",
    "        shaded &= proj >= 0\n",
    "        del length\n",
    "        \n",
    "        dist = proj\n",
    "        dist[:] = l_dir[0, ..., None] * R[1]\n",
    "        dist -= l_dir[1, ..., None] * R[0]\n",
    "        dist.abs_()\n",
    "        \n",
    "        half_width = (width / 2).type(dtype)\n",
    "        shaded &= dist <= half_width[..., None]\n",
    "        del dist, proj, R, l_dir, half_width\n",
    "        \n",
    "        if sparse:\n",
    "            ids = torch.nonzero(shaded).t().contiguous()\n",
    "            del shaded\n",
    "            if samples is None:\n",
    "                samples = torch.sparse.ByteTensor(ids, torch.ones(1, dtype=torch.uint8, device=width.device).expand(ids.shape[1]))\n",
    "                del ids\n",
    "                return samples\n",
    "            else:\n",
    "                pass # TODO\n",
    "        else:\n",
    "            if samples is None:\n",
    "                return shaded\n",
    "            else:\n",
    "                samples += shaded\n",
    "                del shaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define raster grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 64, 64\n",
    "padding = 3\n",
    "padded_h = h + padding*2\n",
    "padded_w = w + padding*2\n",
    "supersampling_strategy = RegularSupersampling(4)\n",
    "pixel_center_coodinates_are_integer = False\n",
    "\n",
    "raster_coordinates = torch.meshgrid(torch.arange(-padding, h+padding, dtype=dtype), torch.arange(-padding, w+padding, dtype=dtype))\n",
    "raster_coordinates = torch.stack([raster_coordinates[1].reshape(-1), raster_coordinates[0].reshape(-1)])\n",
    "if not pixel_center_coodinates_are_integer:\n",
    "    raster_coordinates += .5\n",
    "raster_coordinates = raster_coordinates.to(device)\n",
    "\n",
    "\n",
    "sample_coordinates = supersampling_strategy.supersample(raster_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define default torch renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_lines_pt(lines_batch, samples=None, uint=False):\n",
    "    rasters = render_lines(lines_batch[:, :, 0], lines_batch[:, :, 1], lines_batch[:, :, 2], lines_batch[:, :, 3], lines_batch[:, :, 4], sample_coordinates, samples=samples)\n",
    "    # sum all lines\n",
    "    rasters = rasters.sum(1, keepdim=True, dtype=rasters.dtype)\n",
    "    rasters = supersampling_strategy.subsample(rasters).reshape(-1, padded_h, padded_w)\n",
    "    if uint:\n",
    "        return np.uint8(torch.clamp((1 - rasters) * 255, 0, 255).detach().cpu().numpy())\n",
    "    else:\n",
    "        return rasters\n",
    "    \n",
    "    \n",
    "def render_lines_skeleton(lines_batch):\n",
    "    return np.stack(list(map(lambda lines: render_skeleton(lines, [w, h], data_representation='vahe', padding=padding), lines_batch.detach().cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth and initial vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lines_n = 10\n",
    "vector_lines_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lines = np.asarray([get_random_line(h, w) for _ in range(gt_lines_n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lines_n = 2\n",
    "\n",
    "gt_lines = np.asarray([\n",
    "    [32, 60, 32, 4, 3],\n",
    "#    [4, 32, 60, 32, 2]\n",
    "])\n",
    "\n",
    "#gt_lines = np.asarray([\n",
    "#    [10, 30, 90, 30, 3],\n",
    "#    [10, 35, 90, 35, 2]\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_np = render({PT_LINE: gt_lines}, [w, h], data_representation='vahe')\n",
    "raster = torch.from_numpy(raster_np.astype(np.float32)).to(device)\n",
    "rasters_batch = (1 - raster/255)[None]\n",
    "Image.fromarray(raster_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt_lines_batch = torch.from_numpy(gt_lines.astype(np.float32)[None]).to(device)\n",
    "Image.fromarray(render_lines_pt(gt_lines_batch, uint=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vector = np.asarray([get_random_line(h, w) for _ in range(vector_lines_n)])\n",
    "initial_vector = gt_lines + np.random.normal(size=gt_lines.shape) * np.asarray([1, 1, 1, 1, 1/5]) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vector = gt_lines[:2].copy()\n",
    "initial_vector[0] = [10, 20, 40, 40, 1]\n",
    "### initial_vector[1] = [50, 70, 20, 60, 3]\n",
    "#initial_vector[1] = [70, 50, 80, 50, 3.5]\n",
    "#initial_vector[1] = [45, 20, 60, 40, 3.5]\n",
    "#initial_vector[1] = initial_vector[0].copy()\n",
    "#initial_vector[0][3] -= 20\n",
    "#initial_vector[1][1] += 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## #initial_vector = np.asarray([[0, h/2, w, h/2, 1]])\n",
    "## x, y = np.random.random(2) * [w, h]\n",
    "## initial_vector = np.asarray([[x, y, x+1, y+1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_n = len(initial_vector)\n",
    "initial_vector = initial_vector[None]\n",
    "\n",
    "Image.fromarray(render({PT_LINE: initial_vector[0]}, [w, h], data_representation='vahe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "R_close = 1\n",
    "R_far = 32\n",
    "far_weight = 1 / 50\n",
    "close_weight = 1 - far_weight\n",
    "\n",
    "collinearity_beta = 1 / ( (np.abs(np.cos(15 * np.pi / 180)) - 1)**2 )\n",
    "collinearity_field_weight = 2\n",
    "\n",
    "\n",
    "def line_to_point_energy_canonical(line_length, line_halfwidth, point_x, point_y, R):\n",
    "    r\"\"\"Gives the total energy of interaction of a line and a point\n",
    "     for a point-to-point interaction energy given by `-exp(-r**2/R**2)`.\n",
    "    Coordinates of the point `point_x` and `point_y` correspond to the coordiante system\n",
    "     where one end of the line is in the origin and the other is on the positive side of y axis.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.erf((line_length - point_y) / R) + torch.erf(point_y / R)\n",
    "        ) * (\n",
    "        torch.erf((line_halfwidth - point_x) / R) + torch.erf((line_halfwidth + point_x) / R)\n",
    "        ) * (R ** 2)\n",
    "\n",
    "\n",
    "def line_to_point_energy(lines_batch, point_charges):\n",
    "    r\"\"\"For each line in `lines_batch` gives the total energy of interaction of this line with `point_charges`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines_batch : torch.Tensor\n",
    "        of shape [batch_size, lines_n, params_n]\n",
    "    \n",
    "    point_charges : torch.Tensor\n",
    "        of shape [batch_size, lines_n, rasters_n] or [**, rasters_n] broadcastable to this shape\n",
    "    \"\"\"\n",
    "    batch_size, lines_n = lines_batch.shape[:2]\n",
    "    rasters_n = point_charges.shape[-1]\n",
    "    \n",
    "    # Get parameters of the lines\n",
    "    x1 = lines_batch[..., 0]\n",
    "    y1 = lines_batch[..., 1]\n",
    "    x2 = lines_batch[..., 2]\n",
    "    y2 = lines_batch[..., 3]\n",
    "    half_width = lines_batch[..., 4] / 2\n",
    "    lx = x2 - x1\n",
    "    ly = y2 - y1\n",
    "    length = torch.sqrt(lx**2 + ly**2)\n",
    "    nonzero_length = torch.max(length, torch.full([1], division_epsilon, dtype=length.dtype, device=length.device))\n",
    "    lx = lx / nonzero_length\n",
    "    ly = ly / nonzero_length\n",
    "    del lines_batch, x2, y2, nonzero_length\n",
    "    \n",
    "    # Broadcast tensors\n",
    "    x1, y1, half_width, lx, ly, length, point_charges, raster_x, raster_y = torch.broadcast_tensors(x1.unsqueeze(-1), y1.unsqueeze(-1), half_width.unsqueeze(-1), lx.unsqueeze(-1), ly.unsqueeze(-1), length.unsqueeze(-1), point_charges, raster_coordinates[0], raster_coordinates[1])\n",
    "    \n",
    "    # Translate points to canonical coordinate systems of the lines\n",
    "    translated_raster_x = raster_x - x1\n",
    "    translated_raster_y = raster_y - y1\n",
    "    del raster_x, raster_y, x1, y1\n",
    "    canonical_raster_x = translated_raster_x * ly - translated_raster_y * lx\n",
    "    canonical_raster_y = translated_raster_x * lx + translated_raster_y * ly\n",
    "    del translated_raster_x, translated_raster_y, ly, lx\n",
    "    \n",
    "    # Calculate energies per line\n",
    "    energies = line_to_point_energy_canonical(length, half_width, canonical_raster_x, canonical_raster_y, R_close) * close_weight \\\n",
    "             + line_to_point_energy_canonical(length, half_width, canonical_raster_x, canonical_raster_y, R_far) * far_weight\n",
    "    del canonical_raster_x, canonical_raster_y, length, half_width\n",
    "    energies = torch.sum(energies * point_charges, dim=-1)\n",
    "    del point_charges\n",
    "    return energies\n",
    "    \n",
    "\n",
    "def line_to_vector_energy(lines_batch, vector_field):\n",
    "    r\"\"\"For each line in `lines_batch` gives the total energy of interaction of this line with `vector_field`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines_batch : torch.Tensor\n",
    "        of shape [batch_size, lines_n, params_n]\n",
    "    \n",
    "    vector_field : torch.Tensor\n",
    "        of shape [2, batch_size, lines_n, rasters_n]\n",
    "    \"\"\"\n",
    "    batch_size, lines_n = lines_batch.shape[:2]\n",
    "    rasters_n = vector_field.shape[-1]\n",
    "    \n",
    "    # Get parameters of the lines\n",
    "    x1 = lines_batch[..., 0]\n",
    "    y1 = lines_batch[..., 1]\n",
    "    x2 = lines_batch[..., 2]\n",
    "    y2 = lines_batch[..., 3]\n",
    "    half_width = lines_batch[..., 4] / 2\n",
    "    lx = x2 - x1\n",
    "    ly = y2 - y1\n",
    "    length = torch.sqrt(lx**2 + ly**2)\n",
    "    nonzero_length = torch.max(length, torch.full([1], division_epsilon, dtype=length.dtype, device=length.device))\n",
    "    lx = lx / nonzero_length\n",
    "    ly = ly / nonzero_length\n",
    "    del lines_batch, x2, y2, nonzero_length\n",
    "    \n",
    "    # Broadcast tensors\n",
    "    x1, y1, half_width, lx, ly, length, raster_x, raster_y = torch.broadcast_tensors(x1.unsqueeze(-1), y1.unsqueeze(-1), half_width.unsqueeze(-1), lx.unsqueeze(-1), ly.unsqueeze(-1), length.unsqueeze(-1), raster_coordinates[0], raster_coordinates[1])\n",
    "    \n",
    "    # Calculate intensities of vector field interactions and total field interaction intensities\n",
    "    vector_field_norm = torch.norm(vector_field, dim=0)\n",
    "    vector_field_norm += division_epsilon\n",
    "    vector_field /= vector_field_norm\n",
    "    vector_field_interaction_intensities = vector_field[0] * lx + vector_field[1] * ly\n",
    "    del vector_field\n",
    "    vector_field_interaction_intensities = torch.exp(-(vector_field_interaction_intensities.abs() - 1)**2 * collinearity_beta) * vector_field_norm\n",
    "    del vector_field_norm\n",
    "    \n",
    "    # Translate points to canonical coordinate systems of the lines\n",
    "    translated_raster_x = raster_x - x1\n",
    "    translated_raster_y = raster_y - y1\n",
    "    del raster_x, raster_y, x1, y1\n",
    "    canonical_raster_x = translated_raster_x * ly - translated_raster_y * lx\n",
    "    canonical_raster_y = translated_raster_x * lx + translated_raster_y * ly\n",
    "    del translated_raster_x, translated_raster_y, ly, lx\n",
    "    \n",
    "    # Calculate energies per line\n",
    "    energies = line_to_point_energy_canonical(length, half_width, canonical_raster_x, canonical_raster_y, R_close)\n",
    "    del canonical_raster_x, canonical_raster_y, length, half_width\n",
    "    energies = torch.sum(energies * vector_field_interaction_intensities, dim=-1)\n",
    "    del vector_field_interaction_intensities\n",
    "    return energies * collinearity_field_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_field_energy_lines(lines_batch, rasters_batch, empty_charge=empty_charge, close_range_weight=2*(1/.5), elementary_halfwidth=elementary_halfwidth, visibility_padding=visibility_padding):\n",
    "    r\"\"\"...\n",
    "    Algorithm is (for each batch):\n",
    "    1. Render each line on binary supersample grid\n",
    "    2. Sum (OR) individual renderings -- this is total positive charge field\n",
    "    3. For each line calculate the sum from step 2 minus the rendering of this line from step 1\n",
    "       -- this is the excess positive charge field for this line\n",
    "    4. Subsample the renderings from step 3\n",
    "    5. Subtract the actual raster from the subsampled renderings from step 4\n",
    "       -- this is the excess charge field for each line\n",
    "    \n",
    "    Steps 6-8 are needed to avoid local minima.\n",
    "    \n",
    "    6. For each line calculate coordinates of each excess charge in the coordinate system of this line,\n",
    "       where the y axis is aligned along the length, the x axis is aligned along the width,\n",
    "       and the origin is in the center of the line\n",
    "    7. For each line find the largest rectangle aligned along this line and filled with nonempty pixels only\n",
    "    7..Such rectangle can be non unique, so define it like this:\n",
    "    7.1. Select all empty pixels within `elementary_halfwidth` around the direction of the line, i.e |x| <= `elementary_halfwidth`\n",
    "    7.2. Find the  pixels with minimal positive and maximal negative y coordinate\n",
    "         -- these pixels correspond to the 'y' edges of the rectangle\n",
    "    7.3. Select all empty pixels within the 'y' edges of the rectangle\n",
    "    7.4. Find the pixels with minimal absolute x coordinate\n",
    "         -- these pixels correspond to the 'x' edges of the rectangle\n",
    "    8. Weight the excess charge within the rectangle additionally\n",
    "    \n",
    "    9. For each line calculate the energy of its interaction with the excess raster field\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines_batch : torch.Tensor\n",
    "        of shape [batch_size, lines_n, params_n]\n",
    "    \n",
    "    rasters_batch : torch.Tensor\n",
    "        of shape [batch_size, rasters_n]\n",
    "        \n",
    "    close_range_weight : number\n",
    "        Should be twice the inverse of the lowest shading value.\n",
    "    \"\"\"\n",
    "    batch_size, lines_n = lines_batch.shape[:2]\n",
    "    x1 = lines_batch[..., 0]\n",
    "    y1 = lines_batch[..., 1]\n",
    "    x2 = lines_batch[..., 2]\n",
    "    y2 = lines_batch[..., 3]\n",
    "    half_width = lines_batch[..., 4] / 2\n",
    "    lx = x2 - x1\n",
    "    ly = y2 - y1\n",
    "    length = torch.sqrt(lx**2 + ly**2)\n",
    "    nonzero_length = torch.max(length, torch.full([1], division_epsilon, dtype=length.dtype, device=length.device))\n",
    "    lx = lx / nonzero_length\n",
    "    ly = ly / nonzero_length\n",
    "    del nonzero_length\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Render each line on binary supersample grid\n",
    "        individual_rasterizations = render_lines(x1, y1, x2, y2, lines_batch[:, :, 4], sample_coordinates)\n",
    "        \n",
    "        # 2. Sum (OR) individual renderings -- this is total positive charge field\n",
    "        patch_rasterizations = individual_rasterizations.sum(1, dtype=individual_rasterizations.dtype)\n",
    "        \n",
    "        # 3. For each line calculate the sum from step 2 minus the rendering of this line from step 1\n",
    "        #    -- this is the excess positive charge field for this line\n",
    "        others_rasterizations = individual_rasterizations ^ patch_rasterizations.unsqueeze(1)\n",
    "        del patch_rasterizations, individual_rasterizations\n",
    "            \n",
    "        # 4. Subsample the renderings from step 3\n",
    "        others_rasterizations = supersampling_strategy.subsample(others_rasterizations, dtype=dtype)\n",
    "        \n",
    "        # 5. Subtract the actual raster from the subsampled renderings from step 4\n",
    "        #    -- this is the excess charge field for each line\n",
    "        excess_raster = others_rasterizations\n",
    "        rasters_batch = rasters_batch.type(dtype).reshape(batch_size, -1).unsqueeze(1)\n",
    "        excess_raster -= rasters_batch\n",
    "        del others_rasterizations\n",
    "        \n",
    "        # 6. For each line calculate coordinates of each excess charge in the coordinate system of this line,\n",
    "        #    where the y axis is aligned along the length, the x axis is aligned along the width,\n",
    "        #    and the origin is in the center of the line\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        del x1, x2, y1, y2\n",
    "        cx, cy, lx, ly, excess_raster, rasters_batch, raster_x, raster_y = torch.broadcast_tensors(cx.unsqueeze(-1), cy.unsqueeze(-1), lx.unsqueeze(-1), ly.unsqueeze(-1), excess_raster, rasters_batch, raster_coordinates[0], raster_coordinates[1])\n",
    "\n",
    "        # Translate points to canonical coordinate systems of the lines\n",
    "        translated_raster_x = raster_x - cx\n",
    "        translated_raster_y = raster_y - cy\n",
    "        del raster_x, raster_y, cx, cy\n",
    "        canonical_raster_x_abs = translated_raster_x * ly\n",
    "        canonical_raster_x_abs -= translated_raster_y * lx\n",
    "        canonical_raster_x_abs.abs_()\n",
    "        canonical_raster_y = translated_raster_x * lx\n",
    "        canonical_raster_y += translated_raster_y * ly\n",
    "        del translated_raster_x, translated_raster_y, lx, ly\n",
    "    \n",
    "        # 7. For each line find the largest rectangle aligned along this line and filled with nonempty pixels only\n",
    "        #    Such rectangle can be non unique, so define it like this:\n",
    "        # 7.1. Select all empty pixels within `elementary_halfwidth` around the direction of the line, i.e |x| <= `elementary_halfwidth`\n",
    "        # 7.2. Find the  pixels with minimal positive and maximal negative y coordinate\n",
    "        #      -- these pixels correspond to the 'y' edges of the rectangle\n",
    "        empty = rasters_batch <= empty_charge\n",
    "        del rasters_batch\n",
    "        candidates = canonical_raster_x_abs <= elementary_halfwidth\n",
    "        candidates &= empty\n",
    "        points_to_the_right = canonical_raster_y >= 0\n",
    "        candidates_one_side = points_to_the_right & candidates\n",
    "        assert candidates_one_side.any(dim=-1).all(), 'Couldn\\'t find any empty pixel to the right'\n",
    "        max_y = canonical_raster_y.masked_fill(~candidates_one_side, np.inf)\n",
    "        max_y = max_y.min(dim=-1)[0]\n",
    "        candidates_one_side = ~points_to_the_right\n",
    "        del points_to_the_right\n",
    "        candidates_one_side &= candidates\n",
    "        assert candidates_one_side.any(dim=-1).all(), 'Couldn\\'t find any empty pixel to the left'\n",
    "        del candidates\n",
    "        min_y = canonical_raster_y.masked_fill(~candidates_one_side, -np.inf)\n",
    "        del candidates_one_side\n",
    "        min_y = min_y.max(dim=-1)[0]\n",
    "        \n",
    "        # 7.3. Select all empty pixels within the 'y' edges of the rectangle\n",
    "        within_y_edges = canonical_raster_y > min_y.unsqueeze(-1)\n",
    "        within_y_edges &= canonical_raster_y < max_y.unsqueeze(-1)\n",
    "        candidates = within_y_edges & empty\n",
    "        del within_y_edges\n",
    "        \n",
    "        # 7.4. Find the pixels with minimal absolute x coordinate\n",
    "        #      -- these pixels correspond to the 'x' edges of the rectangle\n",
    "        max_x = canonical_raster_x_abs.masked_fill(~candidates, np.inf)\n",
    "        max_x = max_x.min(dim=-1)[0]\n",
    "        max_x.masked_fill_(~torch.isfinite(max_x), 0)\n",
    "        \n",
    "        # 8. Weight the excess charge within the rectangle additionally\n",
    "        #    This is needed to avoid local minima of energy that are not optimal for the size energy\n",
    "        visible_excess_charge = ~empty\n",
    "        del empty\n",
    "        visible_excess_charge &= canonical_raster_y <= (max_y.unsqueeze(-1) + visibility_padding)\n",
    "        del max_y\n",
    "        visible_excess_charge &= canonical_raster_y >= (min_y.unsqueeze(-1) - visibility_padding)\n",
    "        del canonical_raster_y, min_y\n",
    "        visible_excess_charge &= canonical_raster_x_abs <= (max_x.unsqueeze(-1) + visibility_padding)\n",
    "        del canonical_raster_x_abs, max_x\n",
    "        visible_excess_charge &= excess_raster < 0\n",
    "        excess_raster = excess_raster.where(~visible_excess_charge, excess_raster*close_range_weight)\n",
    "        del visible_excess_charge\n",
    "        \n",
    "    # 9. For each line calculate the energy of its interaction with the excess raster field\n",
    "    mean_field_energy = line_to_point_energy(lines_batch, excess_raster).sum(-1).mean()\n",
    "    del excess_raster\n",
    "    return mean_field_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector_field_energy_lines(lines_batch):\n",
    "    r\"\"\"...\n",
    "    Algorithm is (for each batch):\n",
    "    1. Render each line on binary supersample grid and subsample\n",
    "    2. Put unit vector charge, aligned along the line, into every pixel shaded by the line\n",
    "       -- this is its vector charge field\n",
    "    3. Sum individual vector charge fields\n",
    "    4. For each line calculate total complementary vector charge field\n",
    "    5. For each line calculate the energy of its interaction with complementary vector charge field\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines_batch : torch.Tensor\n",
    "        of shape [batch_size, lines_n, params_n]\n",
    "    \"\"\"\n",
    "    batch_size, lines_n = lines_batch.shape[:2]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x1 = lines_batch[..., 0]\n",
    "        y1 = lines_batch[..., 1]\n",
    "        x2 = lines_batch[..., 2]\n",
    "        y2 = lines_batch[..., 3]\n",
    "        \n",
    "        # 1. Render each line on binary supersample grid and subsample\n",
    "        individual_rasterizations = render_lines(x1, y1, x2, y2, lines_batch[:, :, 4], sample_coordinates)\n",
    "        individual_rasterizations = supersampling_strategy.subsample(individual_rasterizations, dtype=dtype)\n",
    "        \n",
    "        # 2. Put unit vector charge, aligned along the line, into every pixel shaded by the line\n",
    "        #    -- this is its vector charge field\n",
    "        l_dir = torch.nn.functional.normalize(torch.stack([x2 - x1, y2 - y1]), dim=0, eps=division_epsilon)\n",
    "        del x1, x2, y1, y2\n",
    "        individual_vector_fields = l_dir.unsqueeze(-1) * individual_rasterizations\n",
    "        del individual_rasterizations, l_dir\n",
    "\n",
    "        # 3. Sum individual vector charge fields\n",
    "        patch_vector_fields = individual_vector_fields.sum(2)\n",
    "    \n",
    "        # 4. For each line calculate total complementary vector charge field\n",
    "        complimentary_vector_fields = patch_vector_fields.unsqueeze(2) - individual_vector_fields\n",
    "        del patch_vector_fields, individual_vector_fields\n",
    "    \n",
    "    # 5. For each line calculate the energy of its interaction with complementary vector charge field\n",
    "    mean_field_energy = line_to_vector_energy(lines_batch, complimentary_vector_fields).sum(-1).mean()\n",
    "    del complimentary_vector_fields\n",
    "    return mean_field_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_energy(lines_batch, rasters_batch, empty_charge=empty_charge, elementary_halfwidth=elementary_halfwidth, visibility_padding=visibility_padding):\n",
    "    r\"\"\"...\n",
    "    Algorithm is (for each batch):\n",
    "    1. Render each line on binary supersample grid\n",
    "    2. Sum (OR) individual renderings -- this is total positive charge field\n",
    "    3. Subsample the total positive charge fields and individual rasterizations\n",
    "    4. Subtract the actual raster from the total positive charge field from step 3\n",
    "       to get the total excess raster for each line\n",
    "    \n",
    "    Steps 5-7 are needed to avoid local minima\n",
    "    \n",
    "    5. For each line calculate coordinates of each excess charge in the coordinate system of this line,\n",
    "       where the y axis is aligned along the length, the x axis is aligned along the width,\n",
    "       and the origin is in the center of the line\n",
    "    6. For each line find the largest rectangle aligned along this line and filled with nonempty pixels only\n",
    "       Such rectangle can be non unique, so define it like this:\n",
    "    6.1. Select all empty pixels within elementary halfwidth around the direction of the line, i.e |x| <= elementary_halfwidth\n",
    "         elementary halfwidth is e.g 1/2\n",
    "    6.2. Find the  pixels with minimal positive and maximal negative y coordinate\n",
    "         -- these pixels correspond to the 'y' edges of the rectangle\n",
    "    6.3. Select all empty pixels within the 'y' edges of the rectangle\n",
    "    6.4. Find the pixels with minimal absolute x coordinate\n",
    "         -- these pixels correspond to the 'x' edges of the rectangle\n",
    "    7. Limit the visible excess charge of this line to the pixels within its rectangle\n",
    "       and add all pixels from its individual subsampled rendering\n",
    "       -- this is the excess charge that the line interacts with\n",
    "    \n",
    "    8. For each line calculate the energy of its interaction with the excess raster field\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines_batch : torch.Tensor\n",
    "        of shape [batch_size, lines_n, params_n]\n",
    "    \n",
    "    rasters_batch : torch.Tensor\n",
    "        of shape [batch_size, rasters_n]\n",
    "    \"\"\"\n",
    "    batch_size, lines_n = lines_batch.shape[:2]\n",
    "    x1 = lines_batch[..., 0]\n",
    "    y1 = lines_batch[..., 1]\n",
    "    x2 = lines_batch[..., 2]\n",
    "    y2 = lines_batch[..., 3]\n",
    "    half_width = lines_batch[..., 4] / 2\n",
    "    lx = x2 - x1\n",
    "    ly = y2 - y1\n",
    "    length = torch.sqrt(lx**2 + ly**2)\n",
    "    nonzero_length = torch.max(length, torch.full([1], division_epsilon, dtype=length.dtype, device=length.device))\n",
    "    lx = lx / nonzero_length\n",
    "    ly = ly / nonzero_length\n",
    "    del nonzero_length\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Render each line on binary supersample grid\n",
    "        individual_rasterizations = render_lines(x1, y1, x2, y2, lines_batch[:, :, 4], sample_coordinates)\n",
    "        \n",
    "        # 2. Sum (OR) individual renderings -- this is total positive charge field\n",
    "        patch_rasterizations = individual_rasterizations.sum(1, dtype=individual_rasterizations.dtype)\n",
    "        \n",
    "        # 3. Subsample the total positive charge fields and individual rasterizations\n",
    "        patch_rasterizations = supersampling_strategy.subsample(patch_rasterizations.unsqueeze(1))\n",
    "        individual_rasterizations = supersampling_strategy.subsample(individual_rasterizations)\n",
    "        \n",
    "        # 4. Subtract the actual raster from the total positive charge field from step 3\n",
    "        #    to get the total excess raster for each line\n",
    "        excess_raster = patch_rasterizations\n",
    "        rasters_batch = rasters_batch.type(dtype).reshape(batch_size, -1).unsqueeze(1)\n",
    "        excess_raster -= rasters_batch\n",
    "        del patch_rasterizations\n",
    "    \n",
    "        # 5. For each line calculate coordinates of each excess charge in the coordinate system of this line,\n",
    "        #    where the y axis is aligned along the length, the x axis is aligned along the width,\n",
    "        #    and the origin is in the center of the line\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        del x1, x2, y1, y2\n",
    "        cx, cy, lx, ly, excess_raster, rasters_batch, raster_x, raster_y = torch.broadcast_tensors(cx.unsqueeze(-1), cy.unsqueeze(-1), lx.unsqueeze(-1), ly.unsqueeze(-1), excess_raster, rasters_batch, raster_coordinates[0], raster_coordinates[1])\n",
    "\n",
    "        # Translate points to canonical coordinate systems of the lines\n",
    "        translated_raster_x = raster_x - cx\n",
    "        translated_raster_y = raster_y - cy\n",
    "        del raster_x, raster_y, cx, cy\n",
    "        canonical_raster_x_abs = translated_raster_x * ly\n",
    "        canonical_raster_x_abs -= translated_raster_y * lx\n",
    "        canonical_raster_x_abs.abs_()\n",
    "        canonical_raster_y = translated_raster_x * lx\n",
    "        canonical_raster_y += translated_raster_y * ly\n",
    "        del translated_raster_x, translated_raster_y, lx, ly\n",
    "    \n",
    "        # 6. For each line find the largest rectangle aligned along this line and filled with nonempty pixels only\n",
    "        #    Such rectangle can be non unique, so define it like this:\n",
    "        # 6.1. Select all empty pixels within elementary halfwidth around the direction of the line, i.e |x| <= elementary_halfwidth\n",
    "        #      elementary halfwidth is e.g 1/2\n",
    "        # 6.2. Find the  pixels with minimal positive and maximal negative y coordinate\n",
    "        #      -- these pixels correspond to the 'y' edges of the rectangle\n",
    "        empty = rasters_batch <= empty_charge\n",
    "        del rasters_batch\n",
    "        candidates = canonical_raster_x_abs <= elementary_halfwidth\n",
    "        candidates &= empty\n",
    "        points_to_the_right = canonical_raster_y >= 0\n",
    "        candidates_one_side = points_to_the_right & candidates\n",
    "        assert candidates_one_side.any(dim=-1).all(), 'Couldn\\'t find any empty pixel to the right'\n",
    "        max_y = canonical_raster_y.masked_fill(~candidates_one_side, np.inf)\n",
    "        max_y = max_y.min(dim=-1)[0]\n",
    "        candidates_one_side = ~points_to_the_right\n",
    "        del points_to_the_right\n",
    "        candidates_one_side &= candidates\n",
    "        assert candidates_one_side.any(dim=-1).all(), 'Couldn\\'t find any empty pixel to the left'\n",
    "        del candidates\n",
    "        min_y = canonical_raster_y.masked_fill(~candidates_one_side, -np.inf)\n",
    "        del candidates_one_side\n",
    "        min_y = min_y.max(dim=-1)[0]\n",
    "        \n",
    "        # 6.3. Select all empty pixels within the 'y' edges of the rectangle\n",
    "        within_y_edges = canonical_raster_y > min_y.unsqueeze(-1)\n",
    "        within_y_edges &= canonical_raster_y < max_y.unsqueeze(-1)\n",
    "        candidates = within_y_edges & empty\n",
    "        del within_y_edges\n",
    "        \n",
    "        # 6.4. Find the pixels with minimal absolute x coordinate\n",
    "        #      -- these pixels correspond to the 'x' edges of the rectangle\n",
    "        max_x = canonical_raster_x_abs.masked_fill(~candidates, np.inf)\n",
    "        max_x = max_x.min(dim=-1)[0]\n",
    "        max_x.masked_fill_(~torch.isfinite(max_x), 0)\n",
    "        \n",
    "        # 7. Limit the visible excess charge of this line to the pixels within its rectangle\n",
    "        #    and add all pixels from its individual subsampled rendering\n",
    "        #    -- this is the excess charge that the line interacts with\n",
    "        visible_excess_charge = ~empty\n",
    "        del empty\n",
    "        visible_excess_charge &= canonical_raster_y <= (max_y.unsqueeze(-1) + visibility_padding)\n",
    "        del max_y\n",
    "        visible_excess_charge &= canonical_raster_y >= (min_y.unsqueeze(-1) - visibility_padding)\n",
    "        del canonical_raster_y, min_y\n",
    "        visible_excess_charge &= canonical_raster_x_abs <= (max_x.unsqueeze(-1) + visibility_padding)\n",
    "        del canonical_raster_x_abs, max_x\n",
    "        excess_raster = excess_raster.where(visible_excess_charge, individual_rasterizations)\n",
    "        del visible_excess_charge, individual_rasterizations\n",
    "        ### ax_debug.imshow(excess_raster[0, 0].detach().cpu().reshape(padded_h, padded_w))\n",
    "        \n",
    "    # 8. For each line calculate the energy of its interaction with the excess raster field\n",
    "    mean_field_energy = line_to_point_energy(lines_batch, excess_raster).sum(-1).mean()\n",
    "    del excess_raster\n",
    "    return mean_field_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialization of bad predictions, snapping, and redundancy filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinit_excess_lines(cx, cy, width, length, excess_raster, patches_to_consider, min_raster_to_fill=.5, min_width=1/8, initial_width=1, initial_length=1):\n",
    "    r\"\"\"\n",
    "    Algorithm is:\n",
    "    1. In each patch find the maximum value of excess raster\n",
    "       that is not already covered by some vector primitive\n",
    "    2. Find the patches for which this value is larger than `min_raster_to_fill`\n",
    "       and that are from `patches_to_consider`\n",
    "       -- only these patches need reinitialization\n",
    "    3. In every such patch find the line with the minimal width\n",
    "    4. Find the patches for which this value is lower than `min_width`\n",
    "         (the patch has invisible lines that are not already 'working')\n",
    "       and that need reinitialization\n",
    "       -- only these patches will be reinitialized\n",
    "    5. In every such patch put the line with the minimal width from step 3\n",
    "       into the position maximum excess raster from step 1\n",
    "       and reinitialize the length and width of this line\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 1. In each patch find the maximum value of excess raster\n",
    "        #    that is not already covered by some vector primitive\n",
    "        max_excess, max_excess_id = torch.max(excess_raster, dim=-1)\n",
    "        # 2. Find the patches for which this value is larger than `min_raster_to_fill`\n",
    "        #    and that are from `patches_to_consider`\n",
    "        #    -- only these patches need reinitialization\n",
    "        patches_to_reinit = torch.tensor(patches_to_consider, dtype=torch.bool, device=cx.device)\n",
    "        patches_to_consider_to_reinit = max_excess >= min_raster_to_fill\n",
    "        patches_to_reinit.masked_scatter_(patches_to_reinit, patches_to_consider_to_reinit)\n",
    "        if patches_to_reinit.sum() == 0:\n",
    "            return\n",
    "        # 3. In every such patch find the line with the minimal width\n",
    "        min_lines_width, lines_to_reinit = torch.min(width[patches_to_reinit], dim=-1)\n",
    "        # 4. Find the patches for which this value is lower than `min_width`\n",
    "        #      (the patch has invisible lines that are not already 'working')\n",
    "        #    and that need reinitialization\n",
    "        #    -- only these patches will be reinitialized\n",
    "        patches_to_reinit_with_excess_lines = min_lines_width < min_width\n",
    "        patches_to_consider_to_reinit[patches_to_consider_to_reinit] &= patches_to_reinit_with_excess_lines\n",
    "        patches_to_reinit[patches_to_reinit] &= patches_to_reinit_with_excess_lines\n",
    "        if patches_to_reinit.sum() == 0:\n",
    "            return\n",
    "        lines_to_reinit = lines_to_reinit[patches_to_reinit_with_excess_lines]\n",
    "        # 5. In every such patch put the line with the minimal width from step 3\n",
    "        #    into the position maximum excess raster from step 1\n",
    "        #    and reinitialize the length and width of this line \n",
    "        cx.data[patches_to_reinit, lines_to_reinit] = raster_coordinates[0, max_excess_id[patches_to_consider_to_reinit]]\n",
    "        cy.data[patches_to_reinit, lines_to_reinit] = raster_coordinates[1, max_excess_id[patches_to_consider_to_reinit]]\n",
    "        length.data[patches_to_reinit, lines_to_reinit] = initial_length\n",
    "        width.data[patches_to_reinit, lines_to_reinit] = initial_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_lines(cx, cy, theta, length, width, pos_optimizer, size_optimizer, width_threshold=1/4, coord_threshold=1.5, direction_threshold=5, min_linear=min_linear):\n",
    "    r\"\"\"\n",
    "    Algorith is for each 'this' line:\n",
    "    1.C. Select the collinear other lines\n",
    "    1.C.21. Among them select the lines with p2 == this p1\n",
    "    1.C.21.W. Among them select the lines with same widths\n",
    "              With the other parts of the algorithm, chances are low that such line is nonunique for 'this' line\n",
    "    2.C.21. Snap the other line with 'this' line, collapse 'this' line,\n",
    "            and keep track of the modified and collapsed lines\n",
    "     Do the same for other variants of 1-2\n",
    "    3. Prevent rocking after snaps      \n",
    "    4. Prevent size freezing after snaps\n",
    "    5. Reset collapsed lines\n",
    "    \"\"\"\n",
    "    modified_lines = torch.full(cx.shape, False, dtype=torch.bool, device=cx.device)\n",
    "    collapsed_lines = torch.full(cx.shape, False, dtype=torch.bool, device=cx.device)\n",
    "    with torch.no_grad():\n",
    "        # For each 'this' line\n",
    "        for line_i in range(cx.shape[1] - 1):\n",
    "            length_others = length[:, line_i+1:]\n",
    "            width_others = width[:, line_i+1:]\n",
    "            theta_others = theta[:, line_i+1:]\n",
    "            cos_others = torch.cos(theta_others)\n",
    "            sin_others = torch.sin(theta_others)\n",
    "            cx_others = cx[:, line_i+1:]\n",
    "            cy_others = cy[:, line_i+1:]\n",
    "            x1_others = cx_others - length_others * cos_others / 2\n",
    "            y1_others = cy_others - length_others * sin_others / 2\n",
    "            x2_others = cx_others + length_others * cos_others / 2\n",
    "            y2_others = cy_others + length_others * sin_others / 2\n",
    "\n",
    "            length_this = length[:, line_i:line_i+1].expand_as(length_others)\n",
    "            width_this = width[:, line_i:line_i+1].expand_as(width_others)\n",
    "            theta_this = theta[:, line_i:line_i+1].expand_as(theta_others)\n",
    "            cos_this = torch.cos(theta_this)\n",
    "            sin_this = torch.sin(theta_this)\n",
    "            cx_this = cx[:, line_i:line_i+1].expand_as(cx_others)\n",
    "            cy_this = cy[:, line_i:line_i+1].expand_as(cy_others)\n",
    "            x1_this = cx_this - length_this * cos_this / 2\n",
    "            y1_this = cy_this - length_this * sin_this / 2\n",
    "            x2_this = cx_this + length_this * cos_this / 2\n",
    "            y2_this = cy_this + length_this * sin_this / 2\n",
    "            \n",
    "            cos_theta_dif = torch.cos(theta_others - theta_this)\n",
    "            cos_threshold = np.cos(direction_threshold * np.pi / 180)\n",
    "            \n",
    "            not_modified_this = torch.full(length_others.shape, True, dtype=torch.bool, device=length_others.device)\n",
    "            \n",
    "            # 1.C. Select the collinear other lines\n",
    "            close_directions = cos_theta_dif > cos_threshold\n",
    "            \n",
    "            # 1.C.21. Among them select the lines with p2 == this p1\n",
    "            close_other2_this1 = close_directions.clone()\n",
    "            close_other2_this1[close_directions] &= torch.abs(x2_others[close_directions] - x1_this[close_directions]) <= coord_threshold\n",
    "            close_other2_this1[close_directions] &= torch.abs(y2_others[close_directions] - y1_this[close_directions]) <= coord_threshold\n",
    "            \n",
    "            # 1.C.21.W. Among them select the lines with same widths\n",
    "            #  With the other parts of the algorithm, chances are low that such line is nonunique for 'this' line\n",
    "            close_width = close_other2_this1.clone()\n",
    "            close_width[close_other2_this1] &= torch.abs(width_others[close_other2_this1] - width_this[close_other2_this1]) < width_threshold\n",
    "\n",
    "            # 2.C.21. Snap the other line with 'this' line,\n",
    "            new_x2_others = x2_this[close_width]\n",
    "            new_y2_others = y2_this[close_width]\n",
    "            cx_others.data[close_width] = (new_x2_others + x1_others[close_width]) / 2\n",
    "            cy_others.data[close_width] = (new_y2_others + y1_others[close_width]) / 2\n",
    "            length_others.data[close_width] = torch.sqrt((new_x2_others - x1_others[close_width])**2 + (new_y2_others - y1_others[close_width])**2) \n",
    "            #         collapse 'this' line\n",
    "            length_this.data[close_width] = min_linear\n",
    "            width_this.data[close_width] = min_linear\n",
    "            #         and keep track of the modified and collapsed lines\n",
    "            modified_lines[:, line_i+1:][close_width] = True\n",
    "            collapsed_lines[:, line_i][close_width.max(dim=-1)[0]] = True\n",
    "            not_modified_this[close_width.max(dim=-1)[0]] = False\n",
    "            close_directions[close_width.max(dim=-1)[0]] = False\n",
    "            del new_x2_others, new_y2_others, close_other2_this1, close_width\n",
    "\n",
    "            # 1.C.12. other's p1 = this p2\n",
    "            close_other1_this2 = close_directions.clone()\n",
    "            close_other1_this2[close_directions] &= torch.abs(x1_others[close_directions] - x2_this[close_directions]) <= coord_threshold\n",
    "            close_other1_this2[close_directions] &= torch.abs(y1_others[close_directions] - y2_this[close_directions]) <= coord_threshold\n",
    "                        \n",
    "            # 1.C.12.W. same widths\n",
    "            close_width = close_other1_this2.clone()\n",
    "            close_width[close_other1_this2] &= torch.abs(width_others[close_other1_this2] - width_this[close_other1_this2]) < width_threshold\n",
    "            \n",
    "            # 2.C.12. Snap the other line with 'this' line,\n",
    "            new_x1_others = x1_this[close_width]\n",
    "            new_y1_others = y1_this[close_width]\n",
    "            cx_others.data[close_width] = (new_x1_others + x2_others[close_width]) / 2\n",
    "            cy_others.data[close_width] = (new_y1_others + y2_others[close_width]) / 2\n",
    "            #         collapse 'this' line\n",
    "            length_others.data[close_width] = torch.sqrt((x2_others[close_width] - new_x1_others)**2 + (y2_others[close_width] - new_y1_others)**2)\n",
    "            length_this.data[close_width] = min_linear\n",
    "            width_this.data[close_width] = min_linear\n",
    "            #         and keep track of the modified and collapsed lines\n",
    "            modified_lines[:, line_i+1:][close_width] = True\n",
    "            collapsed_lines[:, line_i][close_width.max(dim=-1)[0]] = True\n",
    "            not_modified_this[close_width.max(dim=-1)[0]] = False\n",
    "            del new_x1_others, new_y1_others, close_other1_this2, close_width\n",
    "            \n",
    "            # 1.A. anticollinear\n",
    "            close_directions = (cos_theta_dif < -cos_threshold) & not_modified_this\n",
    "            \n",
    "            # 1.A.22. other's p2 = this p2\n",
    "            close_other2_this2 = close_directions.clone()\n",
    "            close_other2_this2[close_directions] &= torch.abs(x2_others[close_directions] - x2_this[close_directions]) <= coord_threshold\n",
    "            close_other2_this2[close_directions] &= torch.abs(y2_others[close_directions] - y2_this[close_directions]) <= coord_threshold\n",
    "            \n",
    "            # 1.A.22.W. same widths\n",
    "            close_width = close_other2_this2.clone()\n",
    "            close_width[close_other2_this2] &= torch.abs(width_others[close_other2_this2] - width_this[close_other2_this2]) < width_threshold\n",
    "\n",
    "            # 2.A.22. Snap the other line with 'this' line,\n",
    "            new_x2_others = x1_this[close_width]\n",
    "            new_y2_others = y1_this[close_width]\n",
    "            cx_others.data[close_width] = (new_x2_others + x1_others[close_width]) / 2\n",
    "            cy_others.data[close_width] = (new_y2_others + y1_others[close_width]) / 2\n",
    "            length_others.data[close_width] = torch.sqrt((new_x2_others - x1_others[close_width])**2 + (new_y2_others - y1_others[close_width])**2) \n",
    "            #         collapse 'this' line\n",
    "            length_this.data[close_width] = min_linear\n",
    "            width_this.data[close_width] = min_linear\n",
    "            #         and keep track of the modified and collapsed lines\n",
    "            modified_lines[:, line_i+1:][close_width] = True\n",
    "            collapsed_lines[:, line_i][close_width.max(dim=-1)[0]] = True\n",
    "            not_modified_this[close_width.max(dim=-1)[0]] = False\n",
    "            close_directions[close_width.max(dim=-1)[0]] = False\n",
    "            del new_x2_others, new_y2_others, close_other2_this2, close_width\n",
    "\n",
    "            # 1.A.11. other's p1 = this p1\n",
    "            close_other1_this1 = close_directions.clone()\n",
    "            close_other1_this1[close_directions] &= torch.abs(x1_others[close_directions] - x1_this[close_directions]) <= coord_threshold\n",
    "            close_other1_this1[close_directions] &= torch.abs(y1_others[close_directions] - y1_this[close_directions]) <= coord_threshold\n",
    "                        \n",
    "            # 1.A.11.W. same widths\n",
    "            close_width = close_other1_this1.clone()\n",
    "            close_width[close_other1_this1] &= torch.abs(width_others[close_other1_this1] - width_this[close_other1_this1]) < width_threshold\n",
    "            \n",
    "            # 2.A.11. Snap the other line with 'this' line,\n",
    "            new_x1_others = x2_this[close_width]\n",
    "            new_y1_others = y2_this[close_width]\n",
    "            cx_others.data[close_width] = (new_x1_others + x2_others[close_width]) / 2\n",
    "            cy_others.data[close_width] = (new_y1_others + y2_others[close_width]) / 2\n",
    "            length_others.data[close_width] = torch.sqrt((x2_others[close_width] - new_x1_others)**2 + (y2_others[close_width] - new_y1_others)**2)\n",
    "            #         collapse 'this' line\n",
    "            length_this.data[close_width] = min_linear\n",
    "            width_this.data[close_width] = min_linear\n",
    "            #         and keep track of the modified and collapsed lines\n",
    "            modified_lines[:, line_i+1:][close_width] = True\n",
    "            collapsed_lines[:, line_i][close_width.max(dim=-1)[0]] = True\n",
    "            del new_x1_others, new_y1_others, close_other1_this1, close_width, close_directions\n",
    "                  \n",
    "    # 3. Prevent rocking after snaps      \n",
    "    angle_damper = pos_optimizer.state[theta]['exp_avg_sq'].new_full([1], 1e6)\n",
    "    pos_optimizer.state[theta]['exp_avg'].data[modified_lines] = 0\n",
    "    pos_optimizer.state[theta]['exp_avg_sq'].data[modified_lines] = pos_optimizer.state[theta]['exp_avg_sq'].data[modified_lines].max(angle_damper)\n",
    "    pos_optimizer.state[cx]['exp_avg'].data[modified_lines] = 0\n",
    "    pos_optimizer.state[cx]['exp_avg_sq'].data[modified_lines] = 0\n",
    "    pos_optimizer.state[cy]['exp_avg'].data[modified_lines] = 0\n",
    "    pos_optimizer.state[cy]['exp_avg_sq'].data[modified_lines] = 0\n",
    "    # 4. Prevent size freezing after snaps\n",
    "    size_optimizer.state[length]['exp_avg'].data[modified_lines] = 0\n",
    "    size_optimizer.state[length]['exp_avg_sq'].data[modified_lines] = 0\n",
    "    size_optimizer.state[width]['exp_avg'].data[modified_lines] = 0\n",
    "    size_optimizer.state[width]['exp_avg_sq'].data[modified_lines] = 0\n",
    "    # 5. Reset collapsed lines\n",
    "    pos_optimizer.state[cx]['exp_avg'].data[collapsed_lines] = 0\n",
    "    pos_optimizer.state[cx]['exp_avg_sq'].data[collapsed_lines] = 0\n",
    "    pos_optimizer.state[cy]['exp_avg'].data[collapsed_lines] = 0\n",
    "    pos_optimizer.state[cy]['exp_avg_sq'].data[collapsed_lines] = 0\n",
    "    pos_optimizer.state[theta]['exp_avg'].data[collapsed_lines] = 0\n",
    "    pos_optimizer.state[theta]['exp_avg_sq'].data[collapsed_lines] = 0\n",
    "    size_optimizer.state[length]['exp_avg'].data[collapsed_lines] = 0\n",
    "    size_optimizer.state[length]['exp_avg_sq'].data[collapsed_lines] = 0\n",
    "    size_optimizer.state[width]['exp_avg'].data[collapsed_lines] = 0\n",
    "    size_optimizer.state[width]['exp_avg_sq'].data[collapsed_lines] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_redundant_lines(cx, cy, theta, length, width, patches_to_consider, enum_type=torch.int8, min_linear=min_linear):\n",
    "    r\"\"\"\n",
    "    Algorithm is:\n",
    "    1. Render each line on binary supersample grid\n",
    "    2. Sum (OR) individual renderings\n",
    "    3. For each line check if the line is redundant:\n",
    "       Remember, that our renderings on the subsample grid are binary.\n",
    "       For each pixel where the current line is rendered we subtract 2 from the total number of lines in this pixel.\n",
    "       If this line is the only one in this pixel, then the result is negative;\n",
    "        if this line is not the only one, then the result is nonnegative.\n",
    "       The line is redundant iff there are no pixels where it is not the only one\n",
    "    4. Remove redundant lines from the total rendering\n",
    "    \"\"\"\n",
    "    # assert lines_n < 128, 'Risk of overflow! Change int8 to something else'\n",
    "    lines_n = cx.shape[1]\n",
    "    with torch.no_grad():\n",
    "        x2 = cx.data[patches_to_consider] + length.data[patches_to_consider] * torch.cos(theta.data[patches_to_consider]) / 2\n",
    "        x1 = cx.data[patches_to_consider] - length.data[patches_to_consider] * torch.cos(theta.data[patches_to_consider]) / 2\n",
    "        y2 = cy.data[patches_to_consider] + length.data[patches_to_consider] * torch.sin(theta.data[patches_to_consider]) / 2\n",
    "        y1 = cy.data[patches_to_consider] - length.data[patches_to_consider] * torch.sin(theta.data[patches_to_consider]) / 2\n",
    "        \n",
    "        # 1. Render each line on binary supersample grid\n",
    "        individual_rasterizations = render_lines(x1, y1, x2, y2, width[patches_to_consider], sample_coordinates).type(enum_type)\n",
    "        \n",
    "        # 2. Sum (OR) individual renderings\n",
    "        patch_rasterizations = individual_rasterizations.sum(1, dtype=enum_type)\n",
    "        \n",
    "        for line_i in range(lines_n):\n",
    "            patches_to_work_with = torch.tensor(patches_to_consider, dtype=torch.bool, device=cx.device)\n",
    "            # 3. Check if a line is redundant.\n",
    "            # Remember, that our renderings on the subsample grid are binary.\n",
    "            # For each pixel where the current line is rendered\n",
    "            # we subtract 2 from the total number of lines in this pixel.\n",
    "            # If this line is the only one in this pixel, then the result is negative;\n",
    "            # if this line is not the only one, then the result is nonnegative.\n",
    "            patch_rasterizations_with_negative_line = individual_rasterizations[:, line_i].clone()\n",
    "            patch_rasterizations_with_negative_line *= -2\n",
    "            patch_rasterizations_with_negative_line += patch_rasterizations\n",
    "            \n",
    "            # The line is redundant iff there are no pixels where it is not the only one\n",
    "            line_is_redundant = (patch_rasterizations_with_negative_line >= 0).all(dim=-1)\n",
    "            del patch_rasterizations_with_negative_line\n",
    "            \n",
    "            # 4. Remove redundant lines from the total rendering\n",
    "            patch_rasterizations[line_is_redundant] -= individual_rasterizations[line_is_redundant, line_i]\n",
    "            patches_to_work_with[patches_to_work_with] = line_is_redundant\n",
    "            width[patches_to_work_with, line_i] = min_linear\n",
    "            length[patches_to_work_with, line_i] = min_linear\n",
    "        del individual_rasterizations, patch_rasterizations, line_is_redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters constraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert padding - 2 > 0\n",
    "\n",
    "def constrain_parameters(cx, cy, theta, length, width, canvas_width, canvas_height, size_optimizer, padding=padding-2, min_linear=min_linear, dwarfness_ratio=1):\n",
    "    r\"\"\"\n",
    "    1. Constrain width and length to be non less than `min_linear` which is nonzero\n",
    "       to prevent 'dying' of the lines (any position of a zero-sized line is optimal)\n",
    "    3. Swap width and length for short and wide 'dwarf' lines\n",
    "    4. Limit positions of the lines to the canvas\n",
    "       Nonzero padding is used to prevent nonstability for the lines trying to fit super-narrow raster\n",
    "       (i.e, with small shading value) at the very edge of the canvas\n",
    "       We use the `size_energy` padding value minus 2\n",
    "       since `size_energy` needs the line to be at least 2 pixels away from the edge\n",
    "    5. Limit the length of the line w.r.t the edges of the canvas\n",
    "    \"\"\"\n",
    "    ## # 0. Reset exponential averages for collapsed lines\n",
    "    ## size_optimizer.state[length]['exp_avg'].data[length <= min_linear] = 0\n",
    "    ## size_optimizer.state[length]['exp_avg_sq'].data[length <= min_linear] = 0\n",
    "    ## size_optimizer.state[width]['exp_avg'].data[width <= min_linear] = 0\n",
    "    ## size_optimizer.state[width]['exp_avg_sq'].data[width <= min_linear] = 0\n",
    "    \n",
    "    # 1. Constrain width and length to be non less than `min_linear` which is nonzero\n",
    "    #    to prevent 'dying' of the lines (any position of a zero-sized line is optimal)\n",
    "    width.data.clamp_(min=min_linear)\n",
    "    length.data.clamp_(min=min_linear)\n",
    "    # 2. Keep theta in [0, 2pi)\n",
    "    theta.data.remainder_(np.pi * 2)\n",
    "    \n",
    "    # 3. Swap width and length for short and wide 'dwarf' lines\n",
    "    dwarf_lines = width.data > length.data * dwarfness_ratio\n",
    "    width.data[dwarf_lines], length.data[dwarf_lines] = length.data[dwarf_lines], width.data[dwarf_lines]\n",
    "    theta.data[dwarf_lines] += np.pi / 2\n",
    "    size_optimizer.state[length]['exp_avg'].data[dwarf_lines], size_optimizer.state[width]['exp_avg'].data[dwarf_lines] = size_optimizer.state[width]['exp_avg'].data[dwarf_lines], size_optimizer.state[length]['exp_avg'].data[dwarf_lines]\n",
    "    size_optimizer.state[length]['exp_avg_sq'].data[dwarf_lines], size_optimizer.state[width]['exp_avg_sq'].data[dwarf_lines] = size_optimizer.state[width]['exp_avg_sq'].data[dwarf_lines], size_optimizer.state[length]['exp_avg_sq'].data[dwarf_lines]\n",
    "    \n",
    "    # 4. Limit positions of the lines to the canvas\n",
    "    #    Nonzero padding is used to prevent nonstability for the lines trying to fit super-narrow raster\n",
    "    #    (i.e, with small shading value) at the very edge of the canvas\n",
    "    #    We use the `size_energy` padding value minus 2\n",
    "    #    since `size_energy` needs the line to be at least 2 pixels away from the edge\n",
    "    cx.data.clamp_(min=-padding, max=canvas_width + padding)\n",
    "    cy.data.clamp_(min=-padding, max=canvas_height + padding)\n",
    "    \n",
    "    # 5. Limit the length of the line w.r.t the edges of the canvas\n",
    "    epsiloned_2cos = torch.abs(torch.cos(theta.data)) / 2 + division_epsilon\n",
    "    epsiloned_2sin = torch.abs(torch.sin(theta.data)) / 2 + division_epsilon\n",
    "    maxl = torch.min(torch.stack([\n",
    "        (cx.data + padding) / epsiloned_2cos, (canvas_width + padding - cx.data) / epsiloned_2cos,\n",
    "        (cy.data + padding) / epsiloned_2sin, (canvas_height + padding - cy.data) / epsiloned_2sin\n",
    "    ], -1), dim=-1)[0]\n",
    "    length.data = torch.min(length.data, maxl).clamp(min=min_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on real predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_real = False\n",
    "if test_real:\n",
    "    take_batches = slice(70, 90)\n",
    "    #take_batches = slice(119, 120)\n",
    "    #take_batches = slice(140, 160)\n",
    "\n",
    "    first_batch_to_take = 550\n",
    "    take_batches_n = 400\n",
    "\n",
    "    rasters_batch = torch.load('/home/ovoinov/Downloads/oleg_images').detach()\n",
    "    take_batches = []\n",
    "    while True:\n",
    "        if rasters_batch[first_batch_to_take].max() > 0:\n",
    "            take_batches.append(first_batch_to_take)\n",
    "        if len(take_batches) >= take_batches_n:\n",
    "            break\n",
    "        first_batch_to_take += 1\n",
    "    print(take_batches[-1])\n",
    "    take_batches = np.asarray(take_batches)\n",
    "    #take_batches = [take_batches[319]]\n",
    "    #take_batches = take_batches[[84, 54, 157, 29, 240, 58, 185, 49, 266, 114, 349, 264, 286, 384, 129, 263, 281, 283, 319, 365]]\n",
    "    #take_batches = take_batches[[163, 286, 350, 281, 263, 287, 162, 384, 349, 285, 284, 85, 319, 324, 185, 366, 365, 383, 259, 49]]\n",
    "    #take_batches = take_batches[[-3]]\n",
    "    take_batches = take_batches[[286, 263, 281, 349, 85, 350, 383, 287, 285, 284, 319, 259, 323, 204, 283, 86, 324, 229, 282, 264]]\n",
    "    #take_batches = take_batches[[6]]\n",
    "    \n",
    "    rasters_batch = rasters_batch[take_batches, 0].type(dtype).to(device)\n",
    "    raster_np = (1 - rasters_batch[0].cpu().numpy()) * 255\n",
    "\n",
    "    initial_vector = torch.load('/home/ovoinov/Downloads/oleg_lines').detach()[take_batches].cpu()\n",
    "    removed_lines = initial_vector[..., -1] < .5 * h\n",
    "    rand_x1 = torch.rand_like(initial_vector[removed_lines, [0]]) * w\n",
    "    rand_y1 = torch.rand_like(initial_vector[removed_lines, [1]]) * h\n",
    "    initial_vector[removed_lines, [0]] = rand_x1\n",
    "    initial_vector[removed_lines, [2]] = rand_x1 + 1\n",
    "    initial_vector[removed_lines, [1]] = rand_y1\n",
    "    initial_vector[removed_lines, [3]] = rand_y1 + 1\n",
    "    initial_vector[removed_lines, [4]] = 2**-8\n",
    "    initial_vector = initial_vector[..., :5].numpy()\n",
    "\n",
    "    Image.fromarray(np.hstack([\n",
    "        np.uint8(np.vstack((1 - rasters_batch.detach().cpu().numpy())*255)),\n",
    "        np.vstack([render({PT_LINE: lines}, [w, h], data_representation='vahe') for lines in initial_vector])\n",
    "    ]))\n",
    "else:\n",
    "    rasters_batch = np.asarray(Image.open('/home/ovoinov/Downloads/2019-10-17 21.12.33.jpg')).astype(np.float32)[..., 0]\n",
    "    rasters_batch = (1 - rasters_batch/255)\n",
    "    rasters_batch = torch.from_numpy(rasters_batch[None]).type(dtype).to(device)\n",
    "    initial_vector = np.random.rand(1, 20, 5) * [w, h, w, h, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters_batch = torch.nn.functional.pad(rasters_batch, [padding, padding, padding, padding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize vector lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_batch = torch.from_numpy(initial_vector).type(dtype).to(device)\n",
    "lines_n = lines_batch.shape[1]\n",
    "\n",
    "# get the canonical parameters\n",
    "x1 = lines_batch[:, :, 0]\n",
    "y1 = lines_batch[:, :, 1]\n",
    "x2 = lines_batch[:, :, 2]\n",
    "y2 = lines_batch[:, :, 3]\n",
    "width = lines_batch[:, :, 4]\n",
    "X = x2 - x1\n",
    "Y = y2 - y1\n",
    "length = torch.sqrt(X**2 + Y**2)\n",
    "theta = torch.atan2(Y, X)\n",
    "cx = (x1 + x2) / 2\n",
    "cy = (y1 + y2) / 2\n",
    "del x1, x2, y1, y2, X, Y\n",
    "\n",
    "for canonical_parameter in cx, cy, theta, length, width:\n",
    "    canonical_parameter.requires_grad = True\n",
    "\n",
    "pos_optimizer = NonanAdam([cx, cy, theta], lr=1e-1)\n",
    "size_optimizer = NonanAdam([length, width], lr=1e-1)\n",
    "# initialize optimizers\n",
    "(cx + cy + theta + length + width).reshape(-1)[0].backward()\n",
    "pos_optimizer.zero_grad()\n",
    "pos_optimizer.step()\n",
    "size_optimizer.zero_grad()\n",
    "size_optimizer.step()\n",
    "\n",
    "patches_to_optimize = np.full(lines_batch.shape[0], True, np.bool) #my_iou_score(vector_rendering, rasters_batch) < .98\n",
    "cx_final = torch.empty_like(cx)\n",
    "cy_final = torch.empty_like(cy)\n",
    "theta_final = torch.empty_like(theta)\n",
    "length_final = torch.empty_like(length)\n",
    "width_final = torch.empty_like(width)\n",
    "lines_batch_final = torch.empty_like(lines_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iters_n = 10000\n",
    "plotting = True\n",
    "log_plotting = False\n",
    "\n",
    "energies = np.full(iters_n, np.inf)\n",
    "if plotting:\n",
    "    ims = []\n",
    "    fig, [initial_pred_ax, initial_skeleton_ax, refined_ax, refined_skeleton_ax, gt_ax, dif_ax] = plt.subplots(1, 6, figsize=[4 * 6, 4 * len(lines_batch)])\n",
    "\n",
    "if log_plotting:\n",
    "    fig = plt.figure(figsize=[4, 4])\n",
    "    log_data = []\n",
    "    log_data_x = []\n",
    "    log_plot_ax = fig.gca()\n",
    "    log_plot, = log_plot_ax.plot(log_data_x, log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/ovoinov/work/3ddl/vectorization/FloorplanVectorization')\n",
    "from vectran.metrics.raster_metrics import iou_score\n",
    "\n",
    "\n",
    "def my_iou_score(vector_rendering, rasters_batch, average=None):\n",
    "    _vector = ((1 - vector_rendering.detach().cpu()).clamp(0, 1) * 255).type(torch.uint8).numpy()\n",
    "    _raster = ((1 - rasters_batch.detach().cpu()).clamp(0, 1) * 255).type(torch.uint8).numpy()\n",
    "    return iou_score(_raster, _vector, binarization='median', average=average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Algorithm is:\n",
    "1. Reinitialize excess predictions\n",
    "2. Optimize mean field energy w.r.t position parameters with fixed size parameters\n",
    "   Apply constraints to the parameters of the lines after this and other parts of iteration\n",
    "3. Optimize mean field energy w.r.t size parameters\n",
    "   with fixed positions of the left points of the lines and their orientations\n",
    "   Penalize collinearity of overlapping lines\n",
    "4. Optimize mean field energy w.r.t size parameters\n",
    "   with fixed positions of the right points of the lines and their orientations\n",
    "   Penalize collinearity of overlapping lines\n",
    "5. Snap lines that have coinciding ends, close orientations and close widths\n",
    "6. Prepare the results for logging\n",
    "7. Find the patches with high enough IOU value and do not optimize them furher\n",
    "\"\"\"\n",
    "\n",
    "vector_rendering = render_lines_pt(lines_batch.detach())\n",
    "if plotting:\n",
    "    im = rasters_batch - vector_rendering\n",
    "    im_initial_pred = vector_rendering.cpu().numpy()\n",
    "    im_refined = im_initial_pred\n",
    "    im_gt = rasters_batch.cpu().numpy()\n",
    "    im_dif = im.cpu().numpy()\n",
    "    im_initial_skeleton = render_lines_skeleton(lines_batch.detach())\n",
    "    im_refined_skeleton = im_initial_skeleton\n",
    "    \n",
    "    initial_pred_plot = initial_pred_ax.imshow(np.vstack(im_initial_pred), vmin=0, vmax=1, cmap='gray_r')\n",
    "    refined_plot = refined_ax.imshow(np.vstack(im_refined), vmin=0, vmax=1, cmap='gray_r')\n",
    "    gt_plot = gt_ax.imshow(np.vstack(im_gt), vmin=0, vmax=1, cmap='gray_r')\n",
    "    dif_plot = dif_ax.imshow(np.vstack(im_dif), vmin=-1, vmax=1, cmap='gray_r')\n",
    "    initial_skeleton_plot = initial_skeleton_ax.imshow(np.vstack(im_initial_skeleton))\n",
    "    refined_skeleton_plot = refined_skeleton_ax.imshow(np.vstack(im_refined_skeleton))\n",
    "    \n",
    "    ax_debug = initial_skeleton_ax\n",
    "\n",
    "\n",
    "its_time_to_stop = [False]\n",
    "def plotting_sigint(*args):\n",
    "    its_time_to_stop[0] = True\n",
    "\n",
    "for i in tqdm(range(iters_n)):\n",
    "    try:\n",
    "        # 1. Reinitialize excess predictions\n",
    "        #    The meaning of `patches_to_optimize` is explained below, in step 7\n",
    "        if i % 20 == 0:\n",
    "            x2 = cx.data + length.data * torch.cos(theta.data) / 2\n",
    "            x1 = cx.data - length.data * torch.cos(theta.data) / 2\n",
    "            y2 = cy.data + length.data * torch.sin(theta.data) / 2\n",
    "            y1 = cy.data - length.data * torch.sin(theta.data) / 2\n",
    "            lines_batch = torch.stack([x1, y1, x2, y2, width.data], -1)\n",
    "            lines_batch.data[..., -1][lines_batch[..., -1] < 1/4] = 0\n",
    "            vector_rendering[patches_to_optimize] = render_lines_pt(lines_batch[patches_to_optimize].detach())\n",
    "            im = rasters_batch[patches_to_optimize].clone()\n",
    "            # the line below is explained in `reinit_excess_lines`\n",
    "            im.masked_fill_(vector_rendering[patches_to_optimize] > 0, 0)\n",
    "            reinit_excess_lines(cx, cy, width, length, im.reshape(im.shape[0], -1), patches_to_consider=patches_to_optimize)\n",
    "\n",
    "        # 2. Optimize mean field energy w.r.t position parameters with fixed size parameters\n",
    "        x2 = cx + length.data * torch.cos(theta) / 2\n",
    "        x1 = cx - length.data * torch.cos(theta) / 2\n",
    "        y2 = cy + length.data * torch.sin(theta) / 2\n",
    "        y1 = cy - length.data * torch.sin(theta) / 2\n",
    "        lines_batch = torch.stack([x1, y1, x2, y2, width.data], -1)\n",
    "        mean_field_energy = mean_field_energy_lines(lines_batch[patches_to_optimize], rasters_batch[patches_to_optimize])\n",
    "        \n",
    "        pos_optimizer.zero_grad()\n",
    "        mean_field_energy.backward()\n",
    "        pos_optimizer.step()\n",
    "        #    Apply constraints to the parameters of the lines after this and other parts of iteration\n",
    "        constrain_parameters(cx, cy, theta, length, width, canvas_width=w, canvas_height=h, size_optimizer=size_optimizer)\n",
    "        \n",
    "        # 3. Optimize mean field energy w.r.t size parameters\n",
    "        #    with fixed positions of the left points of the lines and their orientations\n",
    "        x1 = cx.data - length.data * torch.cos(theta.data) / 2\n",
    "        y1 = cy.data - length.data * torch.sin(theta.data) / 2\n",
    "        x2 = x1 + length * torch.cos(theta.data)\n",
    "        y2 = y1 + length * torch.sin(theta.data)\n",
    "        lines_batch = torch.stack([x1, y1, x2, y2, width], -1)\n",
    "\n",
    "        excess_energy = size_energy(lines_batch[patches_to_optimize], rasters_batch[patches_to_optimize])\n",
    "        #    Penalize collinearity of overlapping lines\n",
    "        collinearity_energy = mean_vector_field_energy_lines(lines_batch[patches_to_optimize])\n",
    "        size_optimizer.zero_grad()\n",
    "        (excess_energy + collinearity_energy).backward()\n",
    "        size_optimizer.step()\n",
    "        \n",
    "        cx.data[patches_to_optimize] = x1.data[patches_to_optimize] + length.data[patches_to_optimize] * torch.cos(theta.data[patches_to_optimize]) / 2\n",
    "        cy.data[patches_to_optimize] = y1.data[patches_to_optimize] + length.data[patches_to_optimize] * torch.sin(theta.data[patches_to_optimize]) / 2\n",
    "        constrain_parameters(cx, cy, theta, length, width, canvas_width=w, canvas_height=h, size_optimizer=size_optimizer)\n",
    "\n",
    "        # 4. Optimize mean field energy w.r.t size parameters\n",
    "        #    with fixed positions of the right points of the lines and their orientations\n",
    "        x2 = cx.data + length.data * torch.cos(theta.data) / 2\n",
    "        y2 = cy.data + length.data * torch.sin(theta.data) / 2\n",
    "        x1 = x2 - length * torch.cos(theta.data)\n",
    "        y1 = y2 - length * torch.sin(theta.data)\n",
    "        lines_batch = torch.stack([x1, y1, x2, y2, width], -1)\n",
    "\n",
    "        excess_energy = size_energy(lines_batch[patches_to_optimize], rasters_batch[patches_to_optimize])\n",
    "        #    Penalize collinearity of overlapping lines\n",
    "        collinearity_energy = mean_vector_field_energy_lines(lines_batch[patches_to_optimize])\n",
    "        size_optimizer.zero_grad()\n",
    "        (excess_energy + collinearity_energy).backward()\n",
    "        size_optimizer.step()\n",
    "         \n",
    "        cx.data[patches_to_optimize] = x2.data[patches_to_optimize] - length.data[patches_to_optimize] * torch.cos(theta.data[patches_to_optimize]) / 2\n",
    "        cy.data[patches_to_optimize] = y2.data[patches_to_optimize] - length.data[patches_to_optimize] * torch.sin(theta.data[patches_to_optimize]) / 2\n",
    "        constrain_parameters(cx, cy, theta, length, width, canvas_width=w, canvas_height=h, size_optimizer=size_optimizer)\n",
    "        \n",
    "        # 5. Snap lines that have coinciding ends, close orientations and close widths\n",
    "        if (i + 1) % 20 == 0:\n",
    "            snap_lines(cx, cy, theta, length, width, pos_optimizer=pos_optimizer, size_optimizer=size_optimizer)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        its_time_to_stop[0] = True\n",
    "    \n",
    "    # 6. Prepare the results for logging\n",
    "    sigint = signal.signal(signal.SIGINT, plotting_sigint)\n",
    "    \n",
    "    if (i % 20 == 0) or its_time_to_stop[0]:\n",
    "        # 6.1. Record the current values of parameters separately\n",
    "        #      The following steps are performed with these separate values and do not affect the parameters being optimized\n",
    "        cx_final[patches_to_optimize] = cx.data[patches_to_optimize]\n",
    "        cy_final[patches_to_optimize] = cy.data[patches_to_optimize]\n",
    "        theta_final[patches_to_optimize] = theta.data[patches_to_optimize]\n",
    "        length_final[patches_to_optimize] = length.data[patches_to_optimize]\n",
    "        width_final[patches_to_optimize] = width.data[patches_to_optimize]\n",
    "\n",
    "        # 6.2. Collapse invisible lines completely\n",
    "        width_final[width_final < 1/4] = 0\n",
    "        \n",
    "        # 6.3. Collapse the lines that don't add new information to the rasterization\n",
    "        collapse_redundant_lines(cx_final, cy_final, theta_final, length_final, width_final, patches_to_consider=patches_to_optimize)\n",
    "        x2 = cx_final.data[patches_to_optimize] + length_final.data[patches_to_optimize] * torch.cos(theta_final.data[patches_to_optimize]) / 2\n",
    "        x1 = cx_final.data[patches_to_optimize] - length_final.data[patches_to_optimize] * torch.cos(theta_final.data[patches_to_optimize]) / 2\n",
    "        y2 = cy_final.data[patches_to_optimize] + length_final.data[patches_to_optimize] * torch.sin(theta_final.data[patches_to_optimize]) / 2\n",
    "        y1 = cy_final.data[patches_to_optimize] - length_final.data[patches_to_optimize] * torch.sin(theta_final.data[patches_to_optimize]) / 2\n",
    "        lines_batch_final[patches_to_optimize] = torch.stack([x1, y1, x2, y2, width_final[patches_to_optimize]], -1)\n",
    "\n",
    "        # 6.4. Render the lines and calculate the difference from the raster\n",
    "        vector_rendering[patches_to_optimize] = render_lines_pt(lines_batch_final[patches_to_optimize])\n",
    "        im = rasters_batch[patches_to_optimize] - vector_rendering[patches_to_optimize]\n",
    "        # 6.5. Optionally, display rendered vector, difference, skeleton of vector\n",
    "        if plotting:\n",
    "            im_refined[patches_to_optimize] = vector_rendering[patches_to_optimize].cpu().numpy()\n",
    "            im_dif[patches_to_optimize] = im.cpu().numpy()\n",
    "            im_refined_skeleton[patches_to_optimize] = render_lines_skeleton(lines_batch_final[patches_to_optimize])\n",
    "            refined_plot.set_array(np.vstack(im_refined))\n",
    "            dif_plot.set_array(np.vstack(im_dif))\n",
    "            refined_skeleton_plot.set_array(np.vstack(im_refined_skeleton))\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            # Optionally, save the images for further export\n",
    "            ims.append([im_refined.copy(), im_dif.copy(), im_refined_skeleton.copy()])\n",
    "\n",
    "        # 6.6. Optionally, calculate the current mean IOU score and plot\n",
    "        if log_plotting:\n",
    "            log_data.append(my_iou_score(vector_rendering, rasters_batch, average='mean'))\n",
    "            log_data_x.append(i)\n",
    "            clear_output(wait=True)\n",
    "            log_plot_ax.set_xlim(0, i)\n",
    "            log_plot.set_xdata(log_data_x)\n",
    "            log_plot.set_ydata(log_data)\n",
    "            print(log_data[-1])\n",
    "            display(fig)\n",
    "\n",
    "        \n",
    "        # 7. Find the patches with high enough IOU value and do not optimize them furher\n",
    "        #if i % 1 == 0:\n",
    "        #    assert lines_batch[torch.isnan(lines_batch)].numel() == 0\n",
    "        #    new_patches_to_optimize = my_iou_score(vector_rendering[patches_to_optimize], rasters_batch[patches_to_optimize]) < .98\n",
    "        #    patches_to_optimize_left = new_patches_to_optimize.sum()\n",
    "        #    print(patches_to_optimize_left)\n",
    "        #    if patches_to_optimize_left == 0:\n",
    "        #        break\n",
    "        #    patches_to_optimize[patches_to_optimize] &= new_patches_to_optimize\n",
    "        #    # stop optimizers momentum\n",
    "        #    size_optimizer.state[length]['exp_avg'][~patches_to_optimize] = 0\n",
    "        #    size_optimizer.state[width]['exp_avg'][~patches_to_optimize] = 0\n",
    "        #    pos_optimizer.state[cx]['exp_avg'][~patches_to_optimize] = 0\n",
    "        #    pos_optimizer.state[cy]['exp_avg'][~patches_to_optimize] = 0\n",
    "        #    pos_optimizer.state[theta]['exp_avg'][~patches_to_optimize] = 0\n",
    "    \n",
    "    signal.signal(signal.SIGINT, sigint)\n",
    "    if its_time_to_stop[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_data_x, log_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig_debug"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig_debug = plt.figure(figsize=(10,10))\n",
    "ax_debug = fig_debug.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_id = 6\n",
    "\n",
    "im_initial_pred = render_lines_pt(torch.from_numpy(initial_vector[patch_id][None]).type(dtype).to(device)).cpu().numpy()\n",
    "im_refined = render_lines_pt(lines_batch_final[patch_id][None].detach()).cpu().numpy()\n",
    "im_gt = rasters_batch[patch_id][None].cpu().numpy()\n",
    "im_dif = im_refined - im_gt\n",
    "im_initial_skeleton = render_lines_skeleton(torch.from_numpy(initial_vector[patch_id][None]))\n",
    "im_refined_skeleton = render_lines_skeleton(lines_batch_final[patch_id][None].detach())\n",
    "\n",
    "fig, [initial_pred_ax, initial_skeleton_ax, refined_ax, refined_skeleton_ax, gt_ax, dif_ax] = plt.subplots(1, 6, figsize=[4 * 6, 4 * 1])\n",
    "\n",
    "initial_pred_plot = initial_pred_ax.imshow(np.vstack(im_initial_pred), vmin=0, vmax=1, cmap='gray_r')\n",
    "refined_plot = refined_ax.imshow(np.vstack(im_refined), vmin=0, vmax=1, cmap='gray_r')\n",
    "gt_plot = gt_ax.imshow(np.vstack(im_gt), vmin=0, vmax=1, cmap='gray_r')\n",
    "dif_plot = dif_ax.imshow(np.vstack(im_dif), vmin=-1, vmax=1, cmap='gray_r')\n",
    "initial_skeleton_plot = initial_skeleton_ax.imshow(np.vstack(im_initial_skeleton))\n",
    "refined_skeleton_plot = refined_skeleton_ax.imshow(np.vstack(im_refined_skeleton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=[4 * 5, 4 * 2])\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(render_lines_pt(lines_batch_final[patch_id, i][None, None].detach()).cpu().numpy()[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = my_iou_score(vector_rendering, rasters_batch)\n",
    "patches_to_draw = np.argsort(ious)[:20]\n",
    "#patches_to_draw = np.random.choice(len(rasters_batch), 20, replace=False)\n",
    "print(patches_to_draw)\n",
    "\n",
    "im_initial_pred = render_lines_pt(torch.from_numpy(initial_vector[patches_to_draw]).type(dtype).to(device)).cpu().numpy()\n",
    "im_refined = vector_rendering[patches_to_draw].cpu().numpy()\n",
    "im_gt = rasters_batch[patches_to_draw].cpu().numpy()\n",
    "im_dif = im_refined - im_gt\n",
    "im_initial_skeleton = render_lines_skeleton(torch.from_numpy(initial_vector[patches_to_draw]))\n",
    "im_refined_skeleton = render_lines_skeleton(lines_batch_final[patches_to_draw].detach())\n",
    "\n",
    "fig, [initial_pred_ax, initial_skeleton_ax, refined_ax, refined_skeleton_ax, gt_ax, dif_ax] = plt.subplots(1, 6, figsize=[4 * 6, 4 * 20])\n",
    "\n",
    "initial_pred_plot = initial_pred_ax.imshow(np.vstack(im_initial_pred), vmin=0, vmax=1, cmap='gray_r')\n",
    "refined_plot = refined_ax.imshow(np.vstack(im_refined), vmin=0, vmax=1, cmap='gray_r')\n",
    "gt_plot = gt_ax.imshow(np.vstack(im_gt), vmin=0, vmax=1, cmap='gray_r')\n",
    "dif_plot = dif_ax.imshow(np.vstack(im_dif), vmin=-1, vmax=1, cmap='gray_r')\n",
    "initial_skeleton_plot = initial_skeleton_ax.imshow(np.vstack(im_initial_skeleton))\n",
    "refined_skeleton_plot = refined_skeleton_ax.imshow(np.vstack(im_refined_skeleton))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(energies)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fps = 30\n",
    "\n",
    "def animate_func(i):\n",
    "    im_refined, im_dif, im_refined_skeleton = ims[i]\n",
    "    refined_plot.set_array(np.vstack(im_refined))\n",
    "    dif_plot.set_array(np.vstack(im_dif))\n",
    "    refined_skeleton_plot.set_array(np.vstack(im_refined_skeleton))\n",
    "    return [initial_pred_plot, initial_skeleton_plot, refined_plot, refined_skeleton_plot, gt_plot, dif_plot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, \n",
    "                               animate_func, \n",
    "                               frames = len(ims));\n",
    "\n",
    "anim.save('/tmp/test_anim.mp4', fps=fps, extra_args=['-vcodec', 'libx264'])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
